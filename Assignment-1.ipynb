{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"myprojectname\"\n",
    "WANDB_ENTITY = \"myname\"\n",
    "DATASET = \"fashion_mnist\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "LOSS = \"cross_entropy\"\n",
    "OPTIMIZER = \"sgd\"\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "BETA = 0.5\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.5\n",
    "EPSILON = 1e-6\n",
    "WEIGHT_DECAY = 0.0\n",
    "WEIGHT_INIT = \"random\"\n",
    "NUM_LAYERS = 4\n",
    "HIDDEN_SIZE = 128\n",
    "ACTIVATION = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNeuralNetwork():\n",
    "    def __init__(self, neurons=HIDDEN_SIZE, hid_layers=NUM_LAYERS, input_size=784, output_size=10, act_func=ACTIVATION, weight_init=WEIGHT_INIT, out_act_func=\"softmax\"):\n",
    "        self.neurons, self.hidden_layers = neurons, hid_layers\n",
    "        self.weights, self.biases = [], []\n",
    "        self.input_size, self.output_size = input_size, output_size\n",
    "        self.activation_function, self.weight_init = act_func, weight_init\n",
    "        self.output_activation_function = out_act_func\n",
    "\n",
    "        self.initialize_weights()\n",
    "        self.initiate_biases()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights.append(np.random.randn(self.input_size, self.neurons))\n",
    "        for _ in range(self.hidden_layers-1):\n",
    "            self.weights.append(np.random.randn(self.neurons, self.neurons))\n",
    "        self.weights.append(np.random.randn(self.neurons, self.output_size))\n",
    "\n",
    "        if self.weight_init == \"xavier\":\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i] / np.sqrt(self.weights[i].shape[0])\n",
    "\n",
    "    def initiate_biases(self):\n",
    "        for _ in range(self.hidden_layers):\n",
    "            self.biases.append(np.random.randn(self.neurons))\n",
    "        self.biases.append(np.random.randn(self.output_size))\n",
    "    \n",
    "    def activation(self, x):\n",
    "        # x is a matrix of size (batch_size, neurons)\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == \"ReLU\":\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "    \n",
    "    def output_activation(self, x):\n",
    "        # x is a matrix of size (batch_size, output_size)\n",
    "        if self.output_activation_function == \"softmax\":\n",
    "            max_x = np.max(x, axis=1)\n",
    "            max_x = max_x.reshape(max_x.shape[0], 1)\n",
    "            exp_x = np.exp(x - max_x)\n",
    "            return exp_x / np.sum(exp_x, axis=1).reshape(exp_x.shape[0], 1)\n",
    "        else:\n",
    "            raise Exception(\"Invalid output activation function\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a matrix of size (batch_size, input_size)\n",
    "        self.pre_activation, self.post_activation = [x], [x]\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            self.pre_activation.append(np.matmul(self.post_activation[-1], self.weights[i]) + self.biases[i])\n",
    "            self.post_activation.append(self.activation(self.pre_activation[-1]))\n",
    "            \n",
    "        self.pre_activation.append(np.matmul(self.post_activation[-1], self.weights[-1]) + self.biases[-1])\n",
    "        self.post_activation.append(self.output_activation(self.pre_activation[-1]))\n",
    "\n",
    "        return self.post_activation[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(loss, y, y_pred):\n",
    "    # y is a matrix of size (batch_size, output_size)\n",
    "    # y_pred is a matrix of size (batch_size, output_size)\n",
    "    if loss == \"cross_entropy\":\n",
    "        return -np.sum(y * np.log(y_pred))\n",
    "    elif loss == \"mean_squared\":\n",
    "        return np.sum((y - y_pred) ** 2) / (2 * y.shape[0])\n",
    "    else:\n",
    "        raise Exception(\"Invalid loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backpropagation():\n",
    "    def __init__(self, nn: FFNeuralNetwork, loss=LOSS, act_func=ACTIVATION):\n",
    "        self.nn, self.loss, self.activation_function = nn, loss, act_func\n",
    "    \n",
    "    def loss_derivative(self, y, y_pred):\n",
    "        # y is a matrix of size (batch_size, output_size)\n",
    "        if self.loss == \"cross_entropy\":\n",
    "            return -y / y_pred\n",
    "        elif self.loss == \"mse\":\n",
    "            return 2 * (y_pred - y)\n",
    "        else:\n",
    "            raise Exception(\"Invalid loss function\")\n",
    "        \n",
    "    def activation_derivative(self, x):\n",
    "        # x is a matrix of size (batch_size, neurons)\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif self.activation_function == \"ReLU\":\n",
    "            return (x > 0).astype(int)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "        \n",
    "    def output_activation_function(self, y, y_pred):\n",
    "        # this is the derivative of the loss function with respect to the pre-activation of the output layer\n",
    "        if self.nn.output_activation_function == \"softmax\":\n",
    "            return y_pred - y\n",
    "        else:\n",
    "            raise Exception(\"Invalid output activation function\")\n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        # y is a matrix of size (batch_size, output_size)\n",
    "        # y_pred is a matrix of size (batch_size, output_size)\n",
    "        self.d_weights, self.d_biases = [], []\n",
    "        self.d_h, self.d_a = [], []\n",
    "\n",
    "        self.d_h.append(self.loss_derivative(y, y_pred))\n",
    "        self.d_a.append(self.output_activation_function(y, y_pred))\n",
    "\n",
    "        for i in range(self.nn.hidden_layers, 0, -1):\n",
    "            self.d_weights.append(np.matmul(self.nn.post_activation[i].T, self.d_a[-1]))\n",
    "            self.d_biases.append(np.sum(self.d_a[-1], axis=0))\n",
    "            self.d_h.append(np.matmul(self.d_a[-1], self.nn.weights[i].T))\n",
    "            self.d_a.append(self.d_h[-1] * self.activation_derivative(self.nn.pre_activation[i]))\n",
    "\n",
    "        self.d_weights.append(np.matmul(self.nn.post_activation[0].T, self.d_a[-1]))\n",
    "        self.d_biases.append(np.sum(self.d_a[-1], axis=0))\n",
    "\n",
    "        self.d_weights.reverse()\n",
    "        self.d_biases.reverse()\n",
    "\n",
    "        return self.d_weights, self.d_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser():\n",
    "    def __init__(self, nn: FFNeuralNetwork, bp:Backpropagation, lr=LEARNING_RATE, optimiser=OPTIMIZER, momentum=MOMENTUM):\n",
    "        self.nn, self.lr, self.optimiser = nn, lr, optimiser\n",
    "        self.momentum = momentum\n",
    "        self.bp = bp\n",
    "        self.history_weights = [np.zeros_like(w) for w in self.nn.weights]\n",
    "        self.history_biases = [np.zeros_like(b) for b in self.nn.biases]\n",
    "\n",
    "    def run(self, d_weights, d_biases):\n",
    "        if(self.optimiser == \"sgd\"):\n",
    "            self.SGD(d_weights, d_biases)\n",
    "        elif(self.optimiser == \"momentum\"):\n",
    "            self.MomentunGD(d_weights, d_biases)\n",
    "        else:\n",
    "            raise Exception(\"Invalid optimiser\")\n",
    "    \n",
    "    def SGD(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.nn.weights[i] -= self.lr * d_weights[i]\n",
    "            self.nn.biases[i] -= self.lr * d_biases[i]\n",
    "\n",
    "    def MomentunGD(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.history_weights[i] = self.momentum * self.history_weights[i] + d_weights[i]\n",
    "            self.history_biases[i] = self.momentum * self.history_biases[i] + d_biases[i]\n",
    "\n",
    "            self.nn.weights[i] -= self.history_weights[i] * self.lr\n",
    "            self.nn.biases[i] -= self.history_biases[i] * self.lr\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "Epoch: 1, Loss: 76933.39328326975\n",
      "Accuracy: 0.6973\n",
      "Epoch: 2, Loss: 63321.80562899262\n",
      "Accuracy: 0.77915\n",
      "Epoch: 3, Loss: 50759.83300491186\n",
      "Accuracy: 0.79045\n",
      "Epoch: 4, Loss: 42656.53882336947\n",
      "Accuracy: 0.80535\n",
      "Epoch: 5, Loss: 36418.252189558436\n",
      "Accuracy: 0.8167333333333333\n",
      "Epoch: 6, Loss: 34175.929024470264\n",
      "Accuracy: 0.8233\n",
      "Epoch: 7, Loss: 35859.699903745546\n",
      "Accuracy: 0.8222\n",
      "Epoch: 8, Loss: 31456.093385507913\n",
      "Accuracy: 0.83385\n",
      "Epoch: 9, Loss: 28949.980994272177\n",
      "Accuracy: 0.8428666666666667\n",
      "Epoch: 10, Loss: 26663.97555244872\n",
      "Accuracy: 0.8485\n"
     ]
    }
   ],
   "source": [
    "def train(x_train, y_train):\n",
    "    # x_train is a matrix of size (batch_size, input_size)\n",
    "    # y_train is a matrix of size (batch_size, output_size) - one-hot encoded\n",
    "\n",
    "    nn = FFNeuralNetwork(input_size=784, output_size=10, hid_layers=4, neurons=512, act_func=\"sigmoid\", out_act_func=\"softmax\")\n",
    "    bp = Backpropagation(nn, loss=\"cross_entropy\", act_func=\"sigmoid\")\n",
    "    opt = Optimiser(nn, bp, lr=0.002, optimiser=\"momentum\", momentum=0.5)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            y_pred = nn.forward(x_batch)\n",
    "            d_weights, d_biases = bp.backward(y_batch, y_pred)\n",
    "            opt.run(d_weights, d_biases)\n",
    "\n",
    "        y_pred = nn.forward(x_train)\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch + 1, loss(\"cross_entropy\", y_train, y_pred)))\n",
    "        print(\"Accuracy: {}\".format(np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_train, axis=1)) / y_train.shape[0]))\n",
    "    \n",
    "    return nn\n",
    "\n",
    "x_train_reshape = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "y_train_reshape = np.zeros((y_train.shape[0], 10))\n",
    "y_train_reshape[np.arange(y_train.shape[0]), y_train] = 1 # one-hot encoding\n",
    "\n",
    "print(x_train_reshape.shape)\n",
    "print(y_train_reshape.shape)\n",
    "\n",
    "nn = train(x_train_reshape, y_train_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7974\n"
     ]
    }
   ],
   "source": [
    "x_test_reshape = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "y_pred = nn.forward(x_test_reshape)\n",
    "print(\"Test Accuracy: {}\".format(np.sum(np.argmax(y_pred, axis=1) == y_test) / y_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
