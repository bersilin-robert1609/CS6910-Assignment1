{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"myprojectname\"\n",
    "WANDB_ENTITY = \"myname\"\n",
    "DATASET = \"fashion_mnist\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "LOSS = \"cross_entropy\"\n",
    "OPTIMIZER = \"sgd\"\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "BETA = 0.5\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.5\n",
    "EPSILON = 1e-6\n",
    "WEIGHT_DECAY = 0.0\n",
    "WEIGHT_INIT = \"random\"\n",
    "NUM_LAYERS = 4\n",
    "HIDDEN_SIZE = 128\n",
    "ACTIVATION = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNeuralNetwork():\n",
    "    def __init__(self, \n",
    "                neurons=HIDDEN_SIZE, \n",
    "                hid_layers=NUM_LAYERS, \n",
    "                input_size=784, \n",
    "                output_size=10, \n",
    "                act_func=ACTIVATION, \n",
    "                weight_init=WEIGHT_INIT, \n",
    "                out_act_func=\"softmax\",\n",
    "                init_toggle=True):\n",
    "                \n",
    "        self.neurons, self.hidden_layers = neurons, hid_layers\n",
    "        self.weights, self.biases = [], []\n",
    "        self.input_size, self.output_size = input_size, output_size\n",
    "        self.activation_function, self.weight_init = act_func, weight_init\n",
    "        self.output_activation_function = out_act_func\n",
    "\n",
    "        if init_toggle:\n",
    "            self.initialize_weights()\n",
    "            self.initiate_biases()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights.append(np.random.randn(self.input_size, self.neurons))\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(self.neurons, self.neurons))\n",
    "        self.weights.append(np.random.randn(self.neurons, self.output_size))\n",
    "\n",
    "        if self.weight_init == \"xavier\":\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i] / np.sqrt(self.weights[i].shape[0])\n",
    "\n",
    "    def initiate_biases(self):\n",
    "        for _ in range(self.hidden_layers):\n",
    "            self.biases.append(np.random.randn(self.neurons))\n",
    "        self.biases.append(np.random.randn(self.output_size))\n",
    "    \n",
    "    def activation(self, x):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == \"ReLU\":\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "    \n",
    "    def output_activation(self, x):\n",
    "        if self.output_activation_function == \"softmax\":\n",
    "            max_x = np.max(x, axis=1)\n",
    "            max_x = max_x.reshape(max_x.shape[0], 1)\n",
    "            exp_x = np.exp(x - max_x)\n",
    "            softmax_mat = exp_x / np.sum(exp_x, axis=1).reshape(exp_x.shape[0], 1)\n",
    "            # change 0s to 1e-10\n",
    "            softmax_mat[softmax_mat == 0] = 1e-10\n",
    "            return softmax_mat\n",
    "        else:\n",
    "            raise Exception(\"Invalid output activation function\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.pre_activation, self.post_activation = [x], [x]\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            self.pre_activation.append(np.matmul(self.post_activation[-1], self.weights[i]) + self.biases[i])\n",
    "            self.post_activation.append(self.activation(self.pre_activation[-1]))\n",
    "            \n",
    "        self.pre_activation.append(np.matmul(self.post_activation[-1], self.weights[-1]) + self.biases[-1])\n",
    "        self.post_activation.append(self.output_activation(self.pre_activation[-1]))\n",
    "\n",
    "        return self.post_activation[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(loss, y, y_pred):\n",
    "    if loss == \"cross_entropy\":\n",
    "        return -np.sum(y * np.log(y_pred))\n",
    "    elif loss == \"mean_squared\":\n",
    "        return np.sum((y - y_pred) ** 2) / 2\n",
    "    else:\n",
    "        raise Exception(\"Invalid loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backpropagation():\n",
    "    def __init__(self, \n",
    "                 nn: FFNeuralNetwork, \n",
    "                 loss=LOSS, \n",
    "                 act_func=ACTIVATION):\n",
    "        \n",
    "        self.nn, self.loss, self.activation_function = nn, loss, act_func\n",
    "    \n",
    "    def loss_derivative(self, y, y_pred):\n",
    "        if self.loss == \"cross_entropy\":\n",
    "            return -y / y_pred\n",
    "        elif self.loss == \"mse\":\n",
    "            return 2 * (y_pred - y)\n",
    "        else:\n",
    "            raise Exception(\"Invalid loss function\")\n",
    "        \n",
    "    def activation_derivative(self, x):\n",
    "        # x is the post-activation value\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return x * (1 - x)\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return 1 - x ** 2\n",
    "        elif self.activation_function == \"ReLU\":\n",
    "            return 1 * (x > 0)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "        \n",
    "    def output_activation_derivative(self, y, y_pred):\n",
    "        # x is the post-activation value\n",
    "        if self.nn.output_activation_function == \"softmax\":\n",
    "            return y_pred - y\n",
    "        else:\n",
    "            raise Exception(\"Invalid output activation function\")\n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        self.d_weights, self.d_biases = [], []\n",
    "        self.d_h, self.d_a = [], []\n",
    "\n",
    "        self.d_h.append(self.loss_derivative(y, y_pred))\n",
    "        self.d_a.append(self.output_activation_derivative(y, y_pred))\n",
    "\n",
    "        for i in range(self.nn.hidden_layers, 0, -1):\n",
    "            self.d_weights.append(np.matmul(self.nn.post_activation[i].T, self.d_a[-1]))\n",
    "            self.d_biases.append(np.sum(self.d_a[-1], axis=0))\n",
    "            self.d_h.append(np.matmul(self.d_a[-1], self.nn.weights[i].T))\n",
    "            self.d_a.append(self.d_h[-1] * self.activation_derivative(self.nn.post_activation[i]))\n",
    "\n",
    "        self.d_weights.append(np.matmul(self.nn.post_activation[0].T, self.d_a[-1]))\n",
    "        self.d_biases.append(np.sum(self.d_a[-1], axis=0))\n",
    "\n",
    "        self.d_weights.reverse()\n",
    "        self.d_biases.reverse()\n",
    "\n",
    "        return self.d_weights, self.d_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser():\n",
    "    def __init__(self, \n",
    "                 nn: FFNeuralNetwork, \n",
    "                 bp:Backpropagation, \n",
    "                 lr=LEARNING_RATE, \n",
    "                 optimiser=OPTIMIZER, \n",
    "                 momentum=MOMENTUM,\n",
    "                 epsilon=EPSILON,\n",
    "                 beta1=BETA1,\n",
    "                 beta2=BETA2, \n",
    "                 t=0):\n",
    "        \n",
    "        self.nn, self.bp, self.lr, self.optimiser = nn, bp, lr, optimiser\n",
    "        self.momentum, self.epsilon, self.beta1, self.beta2 = momentum, epsilon, beta1, beta2\n",
    "        self.h_weights = [np.zeros_like(w) for w in self.nn.weights]\n",
    "        self.h_biases = [np.zeros_like(b) for b in self.nn.biases]\n",
    "        self.hm_weights = [np.zeros_like(w) for w in self.nn.weights]\n",
    "        self.hm_biases = [np.zeros_like(b) for b in self.nn.biases]\n",
    "        self.t = t\n",
    "\n",
    "    def run(self, d_weights, d_biases, y, x):\n",
    "        if(self.optimiser == \"sgd\"):\n",
    "            self.SGD(d_weights, d_biases)\n",
    "        elif(self.optimiser == \"momentum\"):\n",
    "            self.MomentumGD(d_weights, d_biases)\n",
    "        elif(self.optimiser == \"nag\"):\n",
    "            self.NAG(d_weights, d_biases)\n",
    "        elif (self.optimiser == \"nag2\"):\n",
    "            self.NAG2(y, x)\n",
    "        elif(self.optimiser == \"rmsprop\"):\n",
    "            self.RMSProp(d_weights, d_biases)\n",
    "        elif(self.optimiser == \"adam\"):\n",
    "            self.Adam(d_weights, d_biases)\n",
    "        elif (self.optimiser == \"nadam\"):\n",
    "            self.NAdam(d_weights, d_biases)\n",
    "        else:\n",
    "            raise Exception(\"Invalid optimiser\")\n",
    "    \n",
    "    def SGD(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.nn.weights[i] -= self.lr * d_weights[i]\n",
    "            self.nn.biases[i] -= self.lr * d_biases[i]\n",
    "\n",
    "    def MomentumGD(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.h_weights[i] = self.momentum * self.h_weights[i] + d_weights[i]\n",
    "            self.h_biases[i] = self.momentum * self.h_biases[i] + d_biases[i]\n",
    "\n",
    "            self.nn.weights[i] -= self.h_weights[i] * self.lr\n",
    "            self.nn.biases[i] -= self.h_biases[i] * self.lr\n",
    "\n",
    "    def NAG(self, d_weights, d_biases):        \n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.h_weights[i] = self.momentum * self.h_weights[i] + d_weights[i]\n",
    "            self.h_biases[i] = self.momentum * self.h_biases[i] + d_biases[i]\n",
    "\n",
    "            self.nn.weights[i] -= self.lr * (self.momentum * self.h_weights[i] + d_weights[i])\n",
    "            self.nn.biases[i] -= self.lr * (self.momentum * self.h_biases[i] + d_biases[i])\n",
    "\n",
    "    def NAG2(self, y, x):\n",
    "        nn_new = FFNeuralNetwork(neurons = self.nn.neurons,\n",
    "                                 input_size = self.nn.input_size,\n",
    "                                 output_size = self.nn.output_size,\n",
    "                                 hid_layers = self.nn.hidden_layers,\n",
    "                                 act_func = self.nn.activation_function,\n",
    "                                 out_act_func = self.nn.output_activation_function,\n",
    "                                 weight_init = self.nn.weight_init,\n",
    "                                 init_toggle = False)\n",
    "        \n",
    "        bp_new = Backpropagation(nn = nn_new, \n",
    "                                 loss = self.bp.loss,\n",
    "                                 act_func = self.bp.activation_function)\n",
    "        \n",
    "        nn_new.weights = [w - self.momentum * self.h_weights[i] for i, w in enumerate(self.nn.weights)]\n",
    "        nn_new.biases = [b - self.momentum * self.h_biases[i] for i, b in enumerate(self.nn.biases)]\n",
    "\n",
    "        y_pred_new = nn_new.forward(x)\n",
    "        d_weights_new, d_biases_new = bp_new.backward(y, y_pred_new)\n",
    "\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.h_weights[i] = self.momentum * self.h_weights[i] + d_weights_new[i]\n",
    "            self.h_biases[i] = self.momentum * self.h_biases[i] + d_biases_new[i]\n",
    "\n",
    "            self.nn.weights[i] -= self.h_weights[i] * self.lr\n",
    "            self.nn.biases[i] -= self.h_biases[i] * self.lr\n",
    "\n",
    "    def RMSProp(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.h_weights[i] = self.momentum * self.h_weights[i] + (1 - self.momentum) * d_weights[i]**2\n",
    "            self.h_biases[i] = self.momentum * self.h_biases[i] + (1 - self.momentum) * d_biases[i]**2\n",
    "\n",
    "            self.nn.weights[i] -= (self.lr / (np.sqrt(self.h_weights[i]) + self.epsilon)) * d_weights[i]\n",
    "            self.nn.biases[i] -= (self.lr / (np.sqrt(self.h_biases[i]) + self.epsilon)) * d_biases[i]\n",
    "\n",
    "    def Adam(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.hm_weights[i] = self.beta1 * self.hm_weights[i] + (1 - self.beta1) * d_weights[i]\n",
    "            self.hm_biases[i] = self.beta1 * self.hm_biases[i] + (1 - self.beta1) * d_biases[i]\n",
    "\n",
    "            self.h_weights[i] = self.beta2 * self.h_weights[i] + (1 - self.beta2) * d_weights[i]**2\n",
    "            self.h_biases[i] = self.beta2 * self.h_biases[i] + (1 - self.beta2) * d_biases[i]**2\n",
    "\n",
    "            self.hm_weights_hat = self.hm_weights[i] / (1 - self.beta1**(self.t + 1))\n",
    "            self.hm_biases_hat = self.hm_biases[i] / (1 - self.beta1**(self.t + 1))\n",
    "\n",
    "            self.h_weights_hat = self.h_weights[i] / (1 - self.beta2**(self.t + 1))\n",
    "            self.h_biases_hat = self.h_biases[i] / (1 - self.beta2**(self.t + 1))\n",
    "\n",
    "            self.nn.weights[i] -= self.lr * (self.hm_weights_hat / ((np.sqrt(self.h_weights_hat)) + self.epsilon))\n",
    "            self.nn.biases[i] -= self.lr * (self.hm_biases_hat / ((np.sqrt(self.h_biases_hat)) + self.epsilon))\n",
    "\n",
    "    def NAdam(self, d_weights, d_biases):\n",
    "        for i in range(self.nn.hidden_layers + 1):\n",
    "            self.hm_weights[i] = self.beta1 * self.hm_weights[i] + (1 - self.beta1) * d_weights[i]\n",
    "            self.hm_biases[i] = self.beta1 * self.hm_biases[i] + (1 - self.beta1) * d_biases[i]\n",
    "\n",
    "            self.h_weights[i] = self.beta2 * self.h_weights[i] + (1 - self.beta2) * d_weights[i]**2\n",
    "            self.h_biases[i] = self.beta2 * self.h_biases[i] + (1 - self.beta2) * d_biases[i]**2\n",
    "\n",
    "            self.hm_weights_hat = self.hm_weights[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "            self.hm_biases_hat = self.hm_biases[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "\n",
    "            self.h_weights_hat = self.h_weights[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "            self.h_biases_hat = self.h_biases[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "            temp_update_w = self.beta1 * self.hm_weights_hat + ((1 - self.beta1) / (1 - self.beta1 ** (self.t + 1))) * d_weights[i]\n",
    "            temp_update_b = self.beta1 * self.hm_biases_hat + ((1 - self.beta1) / (1 - self.beta1 ** (self.t + 1))) * d_biases[i]\n",
    "\n",
    "            self.nn.weights[i] -= self.lr * (temp_update_w / ((np.sqrt(self.h_weights_hat)) + self.epsilon))\n",
    "            self.nn.biases[i] -= self.lr * (temp_update_b / ((np.sqrt(self.h_biases_hat)) + self.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.WANDB_NOTEBOOK_NAME = \"Assignment1.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'sweep',\n",
    "    'metric': {\n",
    "        'goal': 'maximize',\n",
    "        'name': 'val_accuracy'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.0001, 0.002, 0.005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "        },\n",
    "        'neurons': {\n",
    "            'values': [32, 64, 128, 256, 512, 1024]\n",
    "        },\n",
    "        'hidden_layers': {\n",
    "            'values': [1, 2, 3, 4, 5]\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['ReLU', 'tanh', 'sigmoid']\n",
    "        },\n",
    "        'weight_init': {\n",
    "            'values': ['Xavier', 'random']\n",
    "        },\n",
    "        'optimiser': {\n",
    "            'values': ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']\n",
    "        },\n",
    "        'momentum': {\n",
    "            'values': [0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        'input_size': {\n",
    "            'value': 784\n",
    "        },\n",
    "        'output_size': {\n",
    "            'value': 10\n",
    "        },\n",
    "        'loss': {\n",
    "            'value': 'cross_entropy'\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 10\n",
    "        },\n",
    "        'beta1': {\n",
    "            'value': 0.9\n",
    "        },\n",
    "        'beta2': {\n",
    "            'value': 0.999\n",
    "        },\n",
    "        'output_activation': {\n",
    "            'value': 'softmax'\n",
    "        },\n",
    "        'epsilon': {\n",
    "            'value': 1e-8\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(type):\n",
    "    (x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "    if type == 'train':\n",
    "        x = x.reshape(x.shape[0], 784)\n",
    "        x = x.astype('float32')\n",
    "        x /= 255\n",
    "        # change y to one hot\n",
    "        y = np.eye(10)[y]\n",
    "        return x, y\n",
    "    elif type == 'test':\n",
    "        x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_test /= 255\n",
    "        # change y to one hot\n",
    "        y_test = np.eye(10)[y_test]\n",
    "        return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    x_train, y_train = load_data('train')\n",
    "\n",
    "    run = wandb.init(project=\"sweep-hyperparameters4\")\n",
    "    parameters = wandb.config\n",
    "    run.name = f\"{parameters['activation']}_neurons={parameters['neurons']}_layers={parameters['hidden_layers']}_lr={parameters['learning_rate']}_batch={parameters['batch_size']}_opt={parameters['optimiser']}_mom={parameters['momentum']}_init={parameters['weight_init']}\"\n",
    "    \n",
    "    nn = FFNeuralNetwork(input_size=parameters['input_size'], \n",
    "                         hid_layers=parameters['hidden_layers'], \n",
    "                         neurons=parameters['neurons'], \n",
    "                         output_size=parameters['output_size'], \n",
    "                         act_func=parameters['activation'], \n",
    "                         out_act_func=parameters['output_activation'],\n",
    "                         weight_init=parameters['weight_init'])\n",
    "    bp = Backpropagation(nn=nn, \n",
    "                         loss=parameters['loss'],\n",
    "                         act_func=parameters['activation'])\n",
    "    opt = Optimiser(nn=nn,\n",
    "                    bp=bp,\n",
    "                    lr=parameters['learning_rate'],\n",
    "                    optimiser=parameters['optimiser'],\n",
    "                    momentum=parameters['momentum'],\n",
    "                    epsilon=parameters['epsilon'],\n",
    "                    beta1=parameters['beta1'],\n",
    "                    beta2=parameters['beta2'])\n",
    "    \n",
    "    batch_size = parameters['batch_size']\n",
    "    x_train_act, x_val, y_train_act, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    print(\"Initial Accuracy: {}\".format(np.sum(np.argmax(nn.forward(x_train), axis=1) == np.argmax(y_train, axis=1)) / y_train.shape[0]))\n",
    "\n",
    "    for epoch in range(parameters['epochs']):\n",
    "        for i in range(0, x_train_act.shape[0], batch_size):\n",
    "            x_batch = x_train_act[i:i+batch_size]\n",
    "            y_batch = y_train_act[i:i+batch_size]\n",
    "\n",
    "            y_pred = nn.forward(x_batch)\n",
    "            d_weights, d_biases = bp.backward(y_batch, y_pred)\n",
    "            opt.run(d_weights, d_biases, y_batch, x_batch)\n",
    "        \n",
    "        opt.t += 1\n",
    "\n",
    "        y_pred = nn.forward(x_train_act)\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch + 1, loss(\"cross_entropy\", y_train_act, y_pred)))\n",
    "        print(\"Accuracy: {}\".format(np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_train_act, axis=1)) / y_train_act.shape[0]))\n",
    "\n",
    "        train_loss = loss(\"cross_entropy\", y_train_act, y_pred)\n",
    "        train_accuracy = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_train_act, axis=1)) / y_train_act.shape[0]\n",
    "        val_loss = loss(\"cross_entropy\", y_val, nn.forward(x_val))\n",
    "        val_accuracy = np.sum(np.argmax(nn.forward(x_val), axis=1) == np.argmax(y_val, axis=1)) / y_val.shape[0]\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_id = wandb.sweep(sweep_configuration, project=\"sweep-hyperparameters4\")\n",
    "\n",
    "wandb.agent(wandb_id, function=train, count=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
